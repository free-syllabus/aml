{"cells":[{"cell_type":"markdown","id":"df628ec2-881f-4af6-a200-43f9e5dd2350","metadata":{"id":"df628ec2-881f-4af6-a200-43f9e5dd2350"},"source":["# Using ConvNeXt to recognize Tom and Jerry"]},{"cell_type":"markdown","id":"2579494c-95c5-4f40-aa43-ac6da24bb1f7","metadata":{"id":"2579494c-95c5-4f40-aa43-ac6da24bb1f7"},"source":["We will use [ConvNeXt](https://arxiv.org/abs/2201.03545) and fine-tune it to correctly classify a dataset with Tom and Jerry pictures.\n","\n","ConvNeXt is a family of CNN models including some that are quite a small, fast and lightweight."]},{"cell_type":"markdown","id":"9e9a560c-c820-4d5d-aa23-07f068d5b9c4","metadata":{"id":"9e9a560c-c820-4d5d-aa23-07f068d5b9c4"},"source":["--------------------"]},{"cell_type":"markdown","id":"9b94eda0-02dc-4581-a529-db91d912fc21","metadata":{"id":"9b94eda0-02dc-4581-a529-db91d912fc21"},"source":["Load neseccary packages and libraries"]},{"cell_type":"code","execution_count":null,"id":"347a925c-74fd-4754-94c4-9a7c06a678b5","metadata":{"id":"347a925c-74fd-4754-94c4-9a7c06a678b5"},"outputs":[],"source":["import keras\n","import numpy as np\n","import os\n","from IPython.display import Image\n","import matplotlib.pyplot as plt\n","\n","from keras.applications.convnext import decode_predictions\n","from keras.utils import get_file, load_img, img_to_array\n","from keras.layers import Dense,GlobalAveragePooling2D\n","from keras.models import Model"]},{"cell_type":"markdown","source":["## Load ConvNeXt"],"metadata":{"id":"CL1tbJWymYqk"},"id":"CL1tbJWymYqk"},{"cell_type":"code","execution_count":null,"id":"567d5b51-dabd-4150-8208-96f6a4417156","metadata":{"id":"567d5b51-dabd-4150-8208-96f6a4417156"},"outputs":[],"source":["convnext = keras.applications.ConvNeXtTiny()"]},{"cell_type":"code","execution_count":null,"id":"bb0fa416-32d2-41f5-8e25-73242c6c3403","metadata":{"id":"bb0fa416-32d2-41f5-8e25-73242c6c3403"},"outputs":[],"source":["def prepare_image(file):\n","    img = load_img(file, target_size=(224, 224))\n","    img = img_to_array(img)\n","    img = np.expand_dims(img, axis=0)\n","    return img"]},{"cell_type":"markdown","id":"00221d4e-0a68-4fc8-ac7f-04faafa3fa31","metadata":{"id":"00221d4e-0a68-4fc8-ac7f-04faafa3fa31"},"source":["## Testing ConvNeXt on dog images"]},{"cell_type":"markdown","id":"2c987bed-7cf8-46d4-a521-5ef6d982b4d6","metadata":{"id":"2c987bed-7cf8-46d4-a521-5ef6d982b4d6"},"source":["Lets try some tests on images of different breed of dogs"]},{"cell_type":"code","execution_count":null,"id":"b5692a12-0ed7-4c89-8c2f-2bf836a5d88d","metadata":{"id":"b5692a12-0ed7-4c89-8c2f-2bf836a5d88d"},"outputs":[],"source":["Image(data='https://upload.wikimedia.org/wikipedia/commons/4/4f/German-shepherd-4040871920._%282%29.jpg', width = 300)"]},{"cell_type":"code","execution_count":null,"id":"9fdaa6cc-7cb5-4a10-9f8a-48ad65bae49f","metadata":{"id":"9fdaa6cc-7cb5-4a10-9f8a-48ad65bae49f"},"outputs":[],"source":["preprocessed_image = prepare_image(get_file('German-shepperd.jpg',origin='https://upload.wikimedia.org/wikipedia/commons/4/4f/German-shepherd-4040871920._%282%29.jpg'))\n","predictions = convnext.predict(preprocessed_image)\n","predictions"]},{"cell_type":"markdown","source":["Decode the labels of predictions"],"metadata":{"id":"zHZjFjFsH301"},"id":"zHZjFjFsH301"},{"cell_type":"code","source":["results = decode_predictions(predictions)\n","results"],"metadata":{"id":"Kn64i9-eHGEt"},"id":"Kn64i9-eHGEt","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Another image"],"metadata":{"id":"RkyAGg04oXEw"},"id":"RkyAGg04oXEw"},{"cell_type":"code","execution_count":null,"id":"7e54331f-c59c-4b1d-97b3-5607f03271c3","metadata":{"id":"7e54331f-c59c-4b1d-97b3-5607f03271c3"},"outputs":[],"source":["Image(data='https://upload.wikimedia.org/wikipedia/commons/d/d4/Labrador_Retriever_-_Yellow.JPG', width = 300)"]},{"cell_type":"code","execution_count":null,"id":"be29bc1a-ef1a-46b6-b085-f241e9ee7eb5","metadata":{"id":"be29bc1a-ef1a-46b6-b085-f241e9ee7eb5"},"outputs":[],"source":["preprocessed_image = prepare_image(get_file('Labrador.jpg',origin='https://upload.wikimedia.org/wikipedia/commons/d/d4/Labrador_Retriever_-_Yellow.JPG'))\n","predictions = convnext.predict(preprocessed_image)\n","results = decode_predictions(predictions)\n","results"]},{"cell_type":"markdown","id":"415f8549-2302-4ff0-aae3-01972fe71a5b","metadata":{"id":"415f8549-2302-4ff0-aae3-01972fe71a5b"},"source":["It works pretty well, you can try here some different pictures if you're curious."]},{"cell_type":"markdown","id":"622980b1-da53-4b50-b77f-3806e01b09b8","metadata":{"id":"622980b1-da53-4b50-b77f-3806e01b09b8"},"source":["## TODO - test on Tom and Jerry"]},{"cell_type":"markdown","id":"8b1f3a37-c016-4410-8475-493d39713296","metadata":{"id":"8b1f3a37-c016-4410-8475-493d39713296"},"source":["Now let's test the network on some images of Tom and Jerry. We will work with images of Tom and Jerry.\n","Please use the code above as a template and try to find some images from the cartoon and test the network on them."]},{"cell_type":"code","execution_count":null,"id":"1ee67eac-0f88-4108-9e9b-c74c6f7242ed","metadata":{"id":"1ee67eac-0f88-4108-9e9b-c74c6f7242ed"},"outputs":[],"source":["preprocessed_image = prepare_image() # TODO"]},{"cell_type":"markdown","id":"f34e7b9c-7a12-4ed1-adc0-8da02263a1db","metadata":{"id":"f34e7b9c-7a12-4ed1-adc0-8da02263a1db"},"source":["## Get our custom dataset - Tom & Jerry"]},{"cell_type":"markdown","id":"aa0dff87-a647-40dc-9eb1-a6b83fe68f5a","metadata":{"id":"aa0dff87-a647-40dc-9eb1-a6b83fe68f5a"},"source":["Lets now manipulate ConvNeXt top few layers and employ transfer learning. To do this, we need to train it on some images. We will train it on images of Tom, Jerry, both and neither of them. We will download the pictures from Kaggle: https://www.kaggle.com/datasets/balabaskar/tom-and-jerry-image-classification"]},{"cell_type":"markdown","source":["Download the dataset"],"metadata":{"id":"VMTqSRJhMKc4"},"id":"VMTqSRJhMKc4"},{"cell_type":"code","source":["DATASET = 'balabaskar/tom-and-jerry-image-classification'\n","ZIP_PATH = './tom-and-jerry-image-classification.zip'\n","IMAGES_PATH = './tom_and_jerry/tom_and_jerry'"],"metadata":{"id":"OMmJq9qhLIWL"},"id":"OMmJq9qhLIWL","execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","os.environ['KAGGLE_USERNAME'] = 'evaklimentov'\n","os.environ['KAGGLE_KEY'] = 'c3161c890c8b21e1e5cba18c9a7505c0'\n","\n","!kaggle datasets download -d {DATASET} -p ./"],"metadata":{"id":"Vrmvsy9fLTmq"},"id":"Vrmvsy9fLTmq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import zipfile\n","\n","with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n","    zip_ref.extractall('./')"],"metadata":{"id":"0f7vNffpLnEl"},"id":"0f7vNffpLnEl","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the dataset in a format suitable for training  and testing"],"metadata":{"id":"kTxvOZUGMNB0"},"id":"kTxvOZUGMNB0"},{"cell_type":"code","source":["batch_size = 32\n","img_height = # TODO\n","img_width = # TODO"],"metadata":{"id":"PXAWgYxEMXu7"},"id":"PXAWgYxEMXu7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = keras.utils.image_dataset_from_directory(\n","    IMAGES_PATH,\n","    validation_split=0.2,\n","    subset=\"training\",\n","    seed=42,\n","    label_mode='categorical',\n","    image_size=(img_height, img_width),\n","    batch_size=batch_size)\n","\n","val_ds = keras.utils.image_dataset_from_directory(\n","    IMAGES_PATH,\n","    validation_split=0.2,\n","    subset=\"validation\",\n","    seed=42,\n","    label_mode='categorical',\n","    image_size=(img_height, img_width),\n","    batch_size=batch_size)"],"metadata":{"id":"UGlMORjGMUC8"},"id":"UGlMORjGMUC8","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Have a look at the pictures"],"metadata":{"id":"ePP87PUjM8Ux"},"id":"ePP87PUjM8Ux"},{"cell_type":"code","source":["label_to_text = {\n","    0: 'Jerry',\n","    1: 'Tom',\n","    2: 'none',\n","    3: 'both'\n","}"],"metadata":{"id":"VVxR2OpjXghV"},"id":"VVxR2OpjXghV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(15, 5))\n","for i, (images, labels) in enumerate(train_ds.take(1)):\n","    for j in range(10):\n","        ax = plt.subplot(2, 5, j + 1)\n","        plt.imshow(images[j].numpy().astype(\"uint8\"))\n","        plt.title(f\"Label: {label_to_text[np.argmax(labels[j])]}\")\n","        plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"iQ1e9smFM7qu"},"id":"iQ1e9smFM7qu","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Get the model"],"metadata":{"id":"9lgq9lfBYM-U"},"id":"9lgq9lfBYM-U"},{"cell_type":"code","source":["base_model = keras.applications.ConvNeXtTiny(include_top=False)\n","\n","# add an average pooling layer\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","# add a fully-connected layer\n","x = Dense(1024, activation='relu')(x)\n","# and a another layer to distinguish our classes\n","predictions = Dense(4, activation='softmax')(x)\n","\n","# this is the model we will train\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","# first: train only the top layers (which were randomly initialized)\n","# i.e. freeze all ConvNeXt layers\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# compile the model\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"],"metadata":{"id":"q5gs3g9zYQIs"},"id":"q5gs3g9zYQIs","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"5vjL-1udcKIO"},"id":"5vjL-1udcKIO","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train the model"],"metadata":{"id":"JkTcmFMjdnV9"},"id":"JkTcmFMjdnV9"},{"cell_type":"code","source":["history = model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=3\n",")"],"metadata":{"id":"G0-EySMfdhM1"},"id":"G0-EySMfdhM1","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Predicting one image from the validation dataset"],"metadata":{"id":"_ufzwL90mWoL"},"id":"_ufzwL90mWoL"},{"cell_type":"code","source":["plt.figure(figsize=(6, 3))\n","for images, labels in val_ds.take(1):\n","    sample_image = images[0]\n","    true_label = labels[0]\n","\n","    sample_image = np.expand_dims(sample_image, axis=0)\n","\n","    predictions = model.predict(sample_image)\n","\n","    predicted_class_index = np.argmax(predictions, axis=1)[0]\n","    predicted_class = label_to_text[predicted_class_index]\n","\n","    plt.imshow(sample_image[0].astype(\"uint8\"))\n","    print(np.argmax(true_label))\n","    plt.title(f\"True label: {label_to_text[np.argmax(true_label)]}\\nPredicted label: {predicted_class}\")\n","    plt.axis('off')\n","\n","plt.show()"],"metadata":{"id":"-gKT-bAyj0Hb"},"id":"-gKT-bAyj0Hb","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Predict downloaded image"],"metadata":{"id":"vVy_XxB6mgTM"},"id":"vVy_XxB6mgTM"},{"cell_type":"code","source":["preprocessed_image = prepare_image() # TODO\n","print(\"Predicted label:\", )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uAaWnSv7mfZK","executionInfo":{"status":"ok","timestamp":1712060234653,"user_tz":-120,"elapsed":16,"user":{"displayName":"Eva Klimentov√°","userId":"06837500812801397724"}},"outputId":"bcaca512-395f-46f8-96c1-7ea3b1883ef3"},"id":"uAaWnSv7mfZK","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 44ms/step\n","Predicted label: Tom\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}