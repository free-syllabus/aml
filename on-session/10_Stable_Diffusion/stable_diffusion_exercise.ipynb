{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/free-syllabus/aml/blob/main/on-session/10_Stable_Diffusion/stable_diffusion_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzOFGRNilDG5"
      },
      "source": [
        "# Stable Diffusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di5RYwfllDHC"
      },
      "source": [
        "Credits go to **Pedro Cuenca, Patrick von Platen, Suraj Patil, Jeremy Howard** for creating this notebook, check their whole course if you want to: https://course.fast.ai/Lessons/lesson9.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw3GAtUolDHE"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq diffusers transformers fastcore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ikf_6OIlDHH"
      },
      "source": [
        "## Using Stable Diffusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x78LZgowlDHI"
      },
      "source": [
        "You need to create a Hugging Face token and log in to continue.\n",
        "\n",
        "(or you can use mine: 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 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bk2s_xvJlDHJ"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from fastcore.all import concat\n",
        "from huggingface_hub import notebook_login\n",
        "from PIL import Image\n",
        "\n",
        "logging.disable(logging.WARNING)\n",
        "\n",
        "torch.manual_seed(1)\n",
        "if not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGT9O94clDHL"
      },
      "source": [
        "### Stable Diffusion Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxKzYDsslDHN"
      },
      "source": [
        "[`StableDiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion#diffusers.StableDiffusionPipeline) is an end-to-end [diffusion inference pipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion) that allows you to start generating images with just a few lines of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZaHfUFMlDHN"
      },
      "outputs": [],
      "source": [
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsl6vk8ZsuR8"
      },
      "source": [
        "Let's use the pipeline to generate our first image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlhIvfU2lDHQ"
      },
      "outputs": [],
      "source": [
        "prompt = \"Bear in a bed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvcHEpTelDHR"
      },
      "outputs": [],
      "source": [
        "pipe(prompt).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQv9OHVilDHR"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1024)\n",
        "pipe(prompt).images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYuwUFi2lDHR"
      },
      "source": [
        "Stable Diffusion is based on a progressive denoising algorithm that is able to create a convincing image starting from pure random noise. We can now look at the intermediary denoising results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S58HNjplDHS"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1024)\n",
        "pipe(prompt, num_inference_steps=5).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oqeC6CjlDHS"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1024)\n",
        "pipe(prompt, num_inference_steps=10).images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY2I-W5xlDHS"
      },
      "source": [
        "### Classifier-Free Guidance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f_R3NyMlDHS"
      },
      "outputs": [],
      "source": [
        "def image_grid(imgs, rows, cols):\n",
        "    w,h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbJk03i6lDHX"
      },
      "source": [
        "_Classifier-Free Guidance_ is a method to increase the adherence of the output to the conditioning signal we used (the text).\n",
        "\n",
        "Roughly speaking, the larger the guidance the more the model tries to represent the text prompt. However, large values tend to produce less diversity. The default is `7.5`, which represents a good compromise between variety and fidelity.\n",
        "\n",
        "We can generate multiple images for the same prompt by simply passing a list of prompts instead of a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmNrcIfilDHX"
      },
      "outputs": [],
      "source": [
        "num_rows,num_cols = 4,4\n",
        "prompts = [prompt] * num_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrVDlpFnlDHX"
      },
      "outputs": [],
      "source": [
        "images = concat(pipe(prompts, guidance_scale=g).images for g in [1.1,3,7,14])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK_5qKurlDHY"
      },
      "outputs": [],
      "source": [
        "image_grid(images, rows=num_rows, cols=num_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0rHPLc9lDHY"
      },
      "source": [
        "### Negative prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64n_GZ-blDHY"
      },
      "source": [
        "_Negative prompting_ refers to the use of another prompt (instead of a completely unconditioned generation), and scaling the difference between generations of that prompt and the conditioned generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDQTB9vSlDHY"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1000)\n",
        "prompt = \"Bear in the style of Vermeer\"\n",
        "pipe(prompt).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utzXmsb8lDHZ"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1000)\n",
        "pipe(prompt, negative_prompt=\"blue\").images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-EQKtf2lDHZ"
      },
      "source": [
        "By using the negative prompt we move more towards the direction of the positive prompt, effectively reducing the importance of the negative prompt in our composition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdO4wpaIlDHZ"
      },
      "source": [
        "### Image to Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo3hIh5ElDHa"
      },
      "source": [
        "Even though Stable Diffusion was trained to generate images, and optionally drive the generation using text conditioning, we can use the raw image diffusion process for other tasks.\n",
        "\n",
        "For example, instead of starting from pure noise, we can start from an image an add a certain amount of noise to it. We are replacing the initial steps of the denoising and pretending our image is what the algorithm came up with. Then we continue the diffusion process from that state as usual.\n",
        "\n",
        "This usually preserves the composition although details may change a lot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m5LOy9SlDHa"
      },
      "source": [
        "These operations (provide an initial image, add some noise to it and run diffusion from there) can be automatically performed by a special image to image pipeline: `StableDiffusionImg2ImgPipeline`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjg-GaYLlDHa"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "from fastdownload import FastDownload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSkAsnENlDHa"
      },
      "outputs": [],
      "source": [
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\",\n",
        "    revision=\"fp16\",\n",
        "    torch_dtype=torch.float16,\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl_b5sq7lDHa"
      },
      "source": [
        "We'll use as an example an image from Wikipedia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYo047kClDHb"
      },
      "outputs": [],
      "source": [
        "p = FastDownload().download('https://upload.wikimedia.org/wikipedia/commons/f/f5/Howlsnow.jpg')\n",
        "init_image = Image.open(p).convert(\"RGB\").resize((400, 400))\n",
        "init_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggIe5-eWlDHg"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "prompt = \"Wolf howling at the moon, fantasy style\"\n",
        "images = pipe(prompt=prompt, num_images_per_prompt=3, image=init_image, strength=0.8, num_inference_steps=50).images\n",
        "image_grid(images, rows=1, cols=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTE7S7AblDHg"
      },
      "source": [
        "When we get a composition we like we can use it as the next seed for another prompt and further change the results. For example, let's take the second image above and try to use it to generate something in the style of Van Gogh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjFzYx07lDHh"
      },
      "outputs": [],
      "source": [
        "init_image = images[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rClHinE_lDHh"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(100)\n",
        "prompt = \"Oil painting of wolf howling at the moon by van Gogh\"\n",
        "images = pipe(prompt=prompt, num_images_per_prompt=3, image=init_image, strength=1, num_inference_steps=70).images\n",
        "image_grid(images, rows=1, cols=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys78xf7mlDHp"
      },
      "source": [
        "## Looking inside the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNNSsd94lDHp"
      },
      "source": [
        "The inference pipeline is just a small piece of code that plugs the components together and performs the inference loop.\n",
        "\n",
        "We'll go through the process of loading and plugging the pieces to see how we could have written it ourselves. We'll start by loading all the modules that we need from their pretrained weights.\n",
        "\n",
        "First, we need the text encoder and the tokenizer. These come from the text portion of a standard CLIP model, so we'll use the weights released by Open AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI5kjzBYlDHq"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPTextModel, CLIPTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bZC0jjElDHq"
      },
      "outputs": [],
      "source": [
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uM3421KlDHq"
      },
      "source": [
        "Next we'll load the `autoencoder` and the `unet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnxxPFe4lDHq"
      },
      "outputs": [],
      "source": [
        "from diffusers import AutoencoderKL, UNet2DConditionModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mnk0_Ri2lDHq"
      },
      "outputs": [],
      "source": [
        "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CdLd13NlDHr"
      },
      "source": [
        "In real diffusion process, people don't add the noise to the training samples just randomly, but it's based on something called a `scheduler`.\n",
        "\n",
        "The higher value of timestep means that we add more noise, and the Beta says what amount of noise we are really adding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm9KSoGulDHr"
      },
      "outputs": [],
      "source": [
        "beta_start,beta_end = 0.00085,0.012\n",
        "plt.plot(torch.linspace(beta_start**0.5, beta_end**0.5, 1000) ** 2)\n",
        "plt.xlabel('Timestep')\n",
        "plt.ylabel('β');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz6RxErclDHr"
      },
      "outputs": [],
      "source": [
        "from diffusers import LMSDiscreteScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRo_Vx57lDHr"
      },
      "outputs": [],
      "source": [
        "scheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXAqIuY4lDHs"
      },
      "source": [
        "We now define the parameters we'll use for generation.\n",
        "\n",
        "In contrast with the previous examples, we set `num_inference_steps` to 70 to get an even more defined image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dru7MgpNlDHs"
      },
      "outputs": [],
      "source": [
        "prompt = [\"Bear in a bed\"]\n",
        "\n",
        "height = 512\n",
        "width = 512\n",
        "num_inference_steps = 70\n",
        "guidance_scale = 7.5\n",
        "batch_size = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7aF2EtnlDHs"
      },
      "source": [
        "We tokenize the prompt. The model requires the same number of tokens for every prompt, so padding is used to ensure we meet the required length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYJtNWYnlDHs"
      },
      "outputs": [],
      "source": [
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "text_input['input_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps_m0Ns8lDHs"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(49407)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxbpsXz6lDHs"
      },
      "source": [
        "The attention mask uses zero to represent tokens we are not interested in. These are all of the padding tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZRZMZzNlDHs"
      },
      "outputs": [],
      "source": [
        "text_input['attention_mask']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBaXCUdNlDHt"
      },
      "source": [
        "The text encoder gives us the embeddings for the text prompt we used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi4e94-dlDHt"
      },
      "outputs": [],
      "source": [
        "text_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0].half()\n",
        "text_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF7XtFU1lDHt"
      },
      "source": [
        "We also get the embeddings required to perform unconditional generation, which is achieved with an empty string: the model is free to go in whichever direction it wants as long as it results in a reasonably-looking image. These embeddings will be applied to apply classifier-free guidance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lsd194KElDHt"
      },
      "outputs": [],
      "source": [
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"], padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "uncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0].half()\n",
        "uncond_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcbLJoaflDHw"
      },
      "source": [
        "For classifier-free guidance, we need to do two forward passes. One with the conditioned input (`text_embeddings`), and another with the unconditional embeddings (`uncond_embeddings`). In practice, we can concatenate both into a single batch to avoid doing two forward passes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffkN3Vu0lDHw"
      },
      "outputs": [],
      "source": [
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1hyJ6ywlDHw"
      },
      "source": [
        "To start the denoising process, we start from pure Gaussian (normal) noise. These are our initial latents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzvhR--WlDHw"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(100)\n",
        "latents = torch.randn((batch_size, unet.in_channels, height // 8, width // 8))\n",
        "latents = latents.to(\"cuda\").half() # using half to get lower precision to make it faster\n",
        "latents.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppWo9_1dlDHx"
      },
      "source": [
        "`4×64×64` is the input shape. The decoder will later transform this latent representation into a `3×512×512` image after the denoising process is complete.\n",
        "\n",
        "Next, we initialize the scheduler with our chosen `num_inference_steps`. This will prepare the internal state to be used during denoising."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMvWuGQnlDHx"
      },
      "outputs": [],
      "source": [
        "scheduler.set_timesteps(num_inference_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFjQk_k0lDHx"
      },
      "source": [
        "We scale the initial noise by the standard deviation required by the scheduler. This value will depend on the particular scheduler we use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKOCweXLlDHx"
      },
      "outputs": [],
      "source": [
        "latents = latents * scheduler.init_noise_sigma # because of activations and gradients to not get out of control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBh6u49ElDHx"
      },
      "source": [
        "We are ready to write the denoising loop. The timesteps go from `999` to `0` (1000 steps that were used during training) following a particular schedule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_1IfoPJlDHy"
      },
      "outputs": [],
      "source": [
        "scheduler.timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDZZq3W9lDHy"
      },
      "outputs": [],
      "source": [
        "scheduler.sigmas # actual number of noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8y7pirflDHy"
      },
      "outputs": [],
      "source": [
        "plt.plot(scheduler.timesteps, scheduler.sigmas[:-1]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay204VALlDHy"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvgEK0WVlDHy"
      },
      "outputs": [],
      "source": [
        "for i, t in enumerate(tqdm(scheduler.timesteps)):\n",
        "    input = torch.cat([latents] * 2)\n",
        "    input = scheduler.scale_model_input(input, t)\n",
        "\n",
        "    # predict the noise residual\n",
        "    with torch.no_grad(): pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "    # perform guidance\n",
        "    pred_uncond, pred_text = pred.chunk(2)\n",
        "    pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n",
        "\n",
        "    # compute the \"previous\" noisy sample\n",
        "    latents = scheduler.step(pred, t, latents).prev_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q_zYdsolDHz"
      },
      "source": [
        "After this process complets our `latents` contain the denoised representation of the image. We use the `vae` decoder to convert it back to pixel space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wlc0R2ElDHz"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad(): image = vae.decode(1 / 0.18215 * latents).sample # scaling based on the original paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buDAnrrWlDHz"
      },
      "source": [
        "And finally, let's convert the image to PIL so we can display it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VYZHVvUlDHz"
      },
      "outputs": [],
      "source": [
        "image = (image / 2 + 0.5).clamp(0, 1)\n",
        "image = image[0].detach().cpu().permute(1, 2, 0).numpy()\n",
        "image = (image * 255).round().astype(\"uint8\")\n",
        "Image.fromarray(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tfiA7PtkSYC_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ONAXY5lDH1"
      },
      "source": [
        "## TODO\n",
        "\n",
        "Adjust the following code to generate images not only based on the guidance prompt, but similarly as in section 'Negative prompt' to consider the *negative prompt*. The model will now not scale between *guided image* and *random image*, but between *guided image* and *negatively guided image*.\n",
        "\n",
        "The following code is the same as the one we've just went through, but it's in functions to make it nice and readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Txc-bEIdlDH2"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    'Bear in a bed',\n",
        "    'Oil painting of a bear in a bed'\n",
        "]\n",
        "\n",
        "neg_prompts = [\n",
        "    'brown',\n",
        "    'brown'\n",
        "]"
      ],
      "metadata": {
        "id": "2hT0jof8M8_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhyxeGvTlDH1"
      },
      "outputs": [],
      "source": [
        "def text_enc(prompts, maxlen=None):\n",
        "    if maxlen is None: maxlen = tokenizer.model_max_length\n",
        "    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n",
        "    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n",
        "\n",
        "def mk_img(t):\n",
        "    image = (t/2+0.5).clamp(0,1).detach().cpu().permute(1, 2, 0).numpy()\n",
        "    return Image.fromarray((image*255).round().astype(\"uint8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og91HjEvlDH1"
      },
      "outputs": [],
      "source": [
        "def mk_samples(prompts, g=7.5, seed=100, steps=70):\n",
        "    bs = len(prompts)\n",
        "    text = text_enc(prompts)\n",
        "    uncond = text_enc([\"\"] * bs, text.shape[1])\n",
        "    emb = torch.cat([uncond, text])\n",
        "    if seed: torch.manual_seed(seed)\n",
        "\n",
        "    latents = torch.randn((bs, unet.in_channels, height//8, width//8))\n",
        "    scheduler.set_timesteps(steps)\n",
        "    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n",
        "\n",
        "    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n",
        "        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
        "        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n",
        "        pred = u + g*(t-u)\n",
        "        latents = scheduler.step(pred, ts, latents).prev_sample\n",
        "\n",
        "    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvlT08wElDH2"
      },
      "outputs": [],
      "source": [
        "images = mk_samples(prompts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for img in images: display(mk_img(img))"
      ],
      "metadata": {
        "id": "-6VArHD-QRVl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
